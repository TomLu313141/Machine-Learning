# -*- coding: utf-8 -*-
"""Midterm.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HNq0W6BzpDBiaWacviKHEnj0dWZPWWTi
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data_train = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Datasets/Midterm/train.csv")
data_test = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Datasets/Midterm/test.csv")

print(data_train.head(10),'\n\n')
print(data_train.shape, '\n\n')
print(len(data_train))

"""# Deal with label"""

for index in range(len(data_train)):

  target = data_train['推薦類型'][index]

  if (target == '肌力'):
    data_train['推薦類型'][index] = 0
  elif (target == '柔軟度'):
    data_train['推薦類型'][index] = 1
  elif (target == '心肺'):
    data_train['推薦類型'][index] = 2
  elif (target == '肌耐力'):
    data_train['推薦類型'][index] = 3
  else:
    data_train['推薦類型'][index] = 4

print(data_train.head(10))

drop_list = []

for i in range(len(data_train)):
  if (data_train['推薦類型'][i] == 4):
    drop_list.append(i)

print(drop_list)

data_train = data_train.drop(data_train.index[drop_list])
print(data_train.head(15))

print(len(data_train))

data_train.reset_index(inplace=True, drop=True)

"""# Deal with age"""

print(data_train.head(15))

drop_list = []

for i in range(len(data_train)):
  if (data_train['年齡'][i] > 100) or (data_train['年齡'][i] < 0):
    drop_list.append(i)

print(drop_list)

data_train = data_train.drop(data_train.index[drop_list])
print(data_train.head(15))

print(len(data_train))

data_train.reset_index(inplace=True, drop=True)

"""# Deal with BMI"""

print(data_train.head(15))

drop_list = []

for i in range(len(data_train)):
  if (data_train['BMI'][i] < 0) :
    # data_train['BMI'][i] = data_train['BMI'][i] * (-1)
    drop_list.append(i)

print(drop_list)
data_train = data_train.drop(data_train.index[drop_list])
data_train.reset_index(inplace=True, drop=True)

print(data_train.head(15))

print(len(data_train))

"""# Deal with Interests"""

print(list(data_train))

"""## drop all incorrect"""

drop_list = []

interests = ['拳擊', '器材', '有氧', '舞蹈', '高強度間歇', '瑜伽', '肌力訓練', '燃脂', '減脂瘦身', '增加肌肉', '增強體魄', '舒緩壓力']
for i in interests:
  for j in range(0, len(data_train)):
    if ((data_train[i][j] != 1.0) and (data_train[i][j] != 0.0)):
      drop_list.append(j)
  print(i,"  col detect completed")

drop_list = np.unique(drop_list)
print(drop_list, '\n\n')

print(np.size(drop_list))

data_train = data_train.drop(data_train.index[drop_list])
data_train.reset_index(inplace=True, drop=True)

"""## Keep incorrect, regularize 1 and 0"""

# interests = ['拳擊', '器材', '有氧', '舞蹈', '高強度間歇', '瑜伽', '肌力訓練', '燃脂', '減脂瘦身', '增加肌肉', '增強體魄', '舒緩壓力']
# for i in interests:
#   for j in range(0, len(data_train)):
#     if (data_train[i][j] >= 1.0) :
#       data_train[i][j] = 1.0
#     elif(data_train[i][j] <= 0.0):
#       data_train[i][j] = 0.0
#     else:
#       print('error')
#   print(i,"  col detect completed")

# print(data_train.head(40))

print(len(data_train))

"""# Split train_x and train_x"""

train_y = data_train['推薦類型']

# Make label's type as int
train_y = train_y.astype('int')

print(train_y)

train_x = data_train.drop(['推薦類型'], axis=1)
print(train_x)

"""# Turn dataframe into numpy"""

train_x = train_x.values
train_y = train_y.values
data_test = data_test.values

"""# Split into X_train and X_test"""

# from sklearn.model_selection import train_test_split

# X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.33)

"""# Decision tree use GridSearchCV"""

# from sklearn import tree
# from sklearn.model_selection import cross_val_score

# # Create a logistic regression object with an L2 penalty
# dt = tree.DecisionTreeClassifier()

# # Fit the grid search
# # clf.fit(X_train, y_train)
# dt.fit(train_x, train_y)

# CV_Result = cross_val_score(dt, train_x, train_y, cv=5, n_jobs=-1)
# print(); print(CV_Result)
# print(); print(CV_Result.mean())  

# # train_acc = clf.score(X_train, y_train)
# # print('train accuracy: ', train_acc)

# # test_acc = clf.score(X_test, y_test)
# # print('test accuracy: ', test_acc)

# test_y_predicted = dt.predict(data_test)

"""# Rf use gridsearch"""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC

rfc=RandomForestClassifier()

param_grid = { 
    'n_estimators': [459,460,461],
    # 'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [14,15,16],
    'criterion' :['gini']
    # 'criterion' :['gini', 'entropy']
}

CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 3)
CV_rfc.fit(train_x, train_y)

# Use Cross Validation To Evaluate Model
CV_Result = cross_val_score(CV_rfc, train_x, train_y, cv=3, n_jobs=-1)
print(); print(CV_rfc.best_params_)
print(); print(CV_Result)
print(); print(CV_Result.mean())
print(); print(CV_Result.std())   

# train_acc = CV_rfc.score(X_train, y_train)
# print('train accuracy: ', train_acc)

# test_acc = CV_rfc.score(X_test, y_test)
# print('test accuracy: ', test_acc)

test_y_predicted = CV_rfc.predict(data_test)

"""# SVM grid"""

# from sklearn.model_selection import cross_val_score
# from sklearn.model_selection import GridSearchCV
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import accuracy_score
# from sklearn.svm import SVC
  
# # defining parameter range
# param_grid = {'C': [80,100, 120, 140, 160], 
#               'gamma': [2, 1.5, 1, 0.5],
#               'kernel': ['rbf']} 
  
# svm_grid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 3)
  
# # fitting the model for grid search
# svm_grid.fit(train_x, train_y)

# # Use Cross Validation To Evaluate Model
# CV_Result = cross_val_score(svm_grid, train_x, train_y, cv=3, n_jobs=-1)
# print(); print(svm_grid.best_params_)
# print(); print(CV_Result)
# print(); print(CV_Result.mean())


# test_y_predicted = svm_grid.predict(data_test)

"""# GradientBoost"""

# from sklearn.ensemble import GradientBoostingClassifier
# from sklearn.model_selection import cross_val_score
# from sklearn.model_selection import GridSearchCV
# from sklearn.metrics import accuracy_score
# from sklearn.metrics import precision_score
# from sklearn.metrics import recall_score
# from sklearn.metrics import make_scorer


# parameters = {
#     "learning_rate": [0.26, 0.27, 0.28],
#     "max_depth":[8],
#     "subsample":[1.0],
#     "n_estimators":[18,19,20]
#     }
# # passing the scoring function in the GridSearchCV
# gbc_grid = GridSearchCV(GradientBoostingClassifier(), parameters,cv=3, n_jobs=-1)

# gbc_grid.fit(train_x, train_y)


# CV_Result = cross_val_score(gbc_grid, train_x, train_y, cv=3, n_jobs=-1)
# print(); print(gbc_grid.best_params_)
# print(); print(CV_Result)
# print(); print(CV_Result.mean())


# test_y_predicted = gbc_grid.predict(data_test)

"""# Check test_y_predicted"""

print(test_y_predicted)

test_y = []

for num in test_y_predicted:
  test_y.append(round(num))

# test_y_predicted = result2label(test_y_predicted)
print(test_y)

"""# Export csv"""

list_index = list()

for x in range(len(data_test)):
    list_index.append(x)

import csv
from itertools import zip_longest

# data = [list_index, test_y_predicted]
data = [list_index, test_y]
columns_data = zip_longest(*data)

with open("F14081088_1019.csv","w") as f:
    writer = csv.writer(f)
    writer.writerow(['index', 'Label'])
    writer.writerows(columns_data)